Algoritmi:

Bag of words. Normalizirana verzija. Broje se pojavljivanja svakog tokena unutar nekog dokumenta. Za svaki token gleda se koliko puta se on pojavio unutar dokumenta i onda se ta vrijednost normalizira s velicinom dokumenta. Na taj nacin se postize da kraci dokumenti u kojima jedno pojavljivanje cini veci postotak unutar dokumenta imaju snazniji utjecaj nego dokumenti u kojima se vise puta pojavljuje doticni token.

Vector space algorithm. Vektorizirana verzija.
- broj dokumeneta (redaka) = 18828
- broj rijeci (stupaca) = 136471
- broj elemenata u sparse matrici = 2648308
- gustoca sparce matrice tf * idf = 0,00103068


Binomial independence.

TODO: Za svaku verziju napraviti opsezna testiranja. 

Docker
Digital ocean
Mongo
Redis

Distribution models:
* each client has whole dataset
* each client has a part of dataset

TIME:
* bag_of_words:
for each document -> list appent -> sort whole list -> ~0.19s
for each document -> heappush -> create whole list -> ~0.25
for each document -> heappush -> take first N -> ~0.19s
for each term -> dictionary[document_id] = bag_value -> 0.0s (selected model)

csr_matrix -> keep memory under control

* load nginx balancer i svojstvo reloadanja.
* cython i falcon, uwsgi
* napraviti testove za opterecenje s wrk-om
* za kvalitetan benchmark potrebno je imati odgovarajuci stroj koji moze opteretiti sustav koji se benchmarka, takodjer je potrebno imati kvalitetnu mrezu koja moze podnjeti veliku kolicinu prometa koja se generira prilikom izvodjenja banchmarka

* napraviti testove s uwsgi-om s vise procesa
* napravit testove s uwsgi-om iza nginx-a
