Algoritmi:

Bag of words. Normalizirana verzija. Broje se pojavljivanja svakog tokena unutar nekog dokumenta. Za svaki token gleda se koliko puta se on pojavio unutar dokumenta i onda se ta vrijednost normalizira s velicinom dokumenta. Na taj nacin se postize da kraci dokumenti u kojima jedno pojavljivanje cini veci postotak unutar dokumenta imaju snazniji utjecaj nego dokumenti u kojima se vise puta pojavljuje doticni token.

Vector space algorithm. Vektorizirana verzija.

Binomial independence.

TODO: Za svaku verziju napraviti opsezna testiranja. 

Docker
Digital ocean
Mongo
Redis

Distribution models:
* each client has whole dataset
* each client has a part of dataset

TIME:
* bag_of_words:
for each document -> list appent -> sort whole list -> ~0.19s
for each document -> heappush -> create whole list -> ~0.25
for each document -> heappush -> take first N -> ~0.19s
for each term -> dictionary[document_id] = bag_value -> 0.0s (selected model)

csr_matrix -> keep memory under control
